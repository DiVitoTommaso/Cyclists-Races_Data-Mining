{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55696f5d-eabe-4b87-947a-fa3683465e3b",
   "metadata": {},
   "source": [
    "Import the main libraries for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c02cc8a-7591-484f-b97e-acf1f6f3e81a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:02.470538Z",
     "start_time": "2024-11-02T13:33:02.014308Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sklearn.impute as imp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4e786f-36bb-4d92-a7e7-90f5fdd6062a",
   "metadata": {},
   "source": [
    "Load the datasets to analize them separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9191ea13-e0b5-4453-93c2-bb3e7db5a086",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:23.912575Z",
     "start_time": "2024-11-02T13:33:21.060684Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's make a preliminary analysis to the datasets\n",
    "races_df = pd.read_csv(\"./dataset/races.csv\")\n",
    "cyclist_df = pd.read_csv(\"./dataset/cyclists.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20ab3ae-e183-42c6-a5db-74b3914ed81d",
   "metadata": {},
   "source": [
    "# Ciclist table preliminary analysis\n",
    "The key for cyclist table is column **_url** that is the name of the cyclist, while in races table the column **cyclist** is a foreign key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0d4eaa-57a2-4c46-8089-bbace08bc28d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:23.997389Z",
     "start_time": "2024-11-02T13:33:23.924616Z"
    }
   },
   "outputs": [],
   "source": [
    "def check_key(df, column_name):\n",
    "    # Check if there are no NaN values and if all values are unique\n",
    "    return df[column_name].notna().all() and df[column_name].is_unique\n",
    "\n",
    "\n",
    "def check_in(df1, df2, col1, col2):\n",
    "    return df1[col1].isin(df2[col2]).all()\n",
    "\n",
    "\n",
    "print(\n",
    "    check_key(cyclist_df, \"_url\")\n",
    ")  # This column is a key for clyclist dataset\n",
    "print(\n",
    "    check_in(races_df, cyclist_df, \"cyclist\", \"_url\")\n",
    ")  # These columns can be used for join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9c52c9-6fda-4d22-9fac-68005eedd8da",
   "metadata": {},
   "source": [
    "Some columns have missing values in the cyclist table. For birth_year and nationality we can try to fix them, while we cannot fix weight and height columns since they are NaN for half of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eea336-43a8-4d19-ae8d-7377bbbed957",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:27.381914Z",
     "start_time": "2024-11-02T13:33:27.352546Z"
    }
   },
   "outputs": [],
   "source": [
    "cyclist_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6b2cc4-3b58-4508-b905-b399b2d4ed91",
   "metadata": {},
   "source": [
    "Only cyclist **scott-davies** has missing nationality. We can fix its nationality and birth_year by looking on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7940ed00-fdd7-4436-b55d-dbfa3e1602b8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:29.559753Z",
     "start_time": "2024-11-02T13:33:29.520621Z"
    }
   },
   "outputs": [],
   "source": [
    "# Only one cyclist has nan nationality.\n",
    "print(\n",
    "    cyclist_df.loc[cyclist_df.nationality[cyclist_df.nationality.isna()].index]\n",
    ")\n",
    "\n",
    "\n",
    "def fix_cyclist(df):\n",
    "    mask = df[\"nationality\"].isna()\n",
    "    cyclists = set(df.loc[mask, \"_url\"])\n",
    "    for c in cyclists:\n",
    "        mask = df[\"_url\"] == c\n",
    "        df.loc[mask, \"nationality\"] = \"Britain\"\n",
    "        df.loc[mask, \"birth_year\"] = 1995\n",
    "\n",
    "\n",
    "# Fix using Wikipedia\n",
    "fix_cyclist(cyclist_df)\n",
    "print(\n",
    "    cyclist_df.loc[\n",
    "        cyclist_df.nationality[cyclist_df._url == \"scott-davies\"].index\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88da566-2c3c-4366-a358-5d53f84b1def",
   "metadata": {},
   "source": [
    "These cyclist instead have missing birth_year, they are few we can try to fix them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94d49a37-15a6-48d4-9d53-7ddb630abeb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:33.321491Z",
     "start_time": "2024-11-02T13:33:33.284525Z"
    }
   },
   "outputs": [],
   "source": [
    "cyclist_df.loc[cyclist_df.birth_year[cyclist_df.birth_year.isna()].index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75d2fec",
   "metadata": {},
   "source": [
    "Eliminate cyclists that do not participate in any race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c48697",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_merge_df = pd.merge(\n",
    "    races_df, cyclist_df, left_on=\"cyclist\", right_on=\"_url\", how=\"outer\"\n",
    ")\n",
    "bad_cyclists = nan_merge_df.loc[nan_merge_df._url_x.isna(), \"_url_y\"].unique()\n",
    "\n",
    "print(cyclist_df.shape)\n",
    "cyclist_df = cyclist_df.loc[~cyclist_df._url.isin(bad_cyclists)]\n",
    "cyclist_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21853d2-b766-4533-80d7-23c810acba3c",
   "metadata": {},
   "source": [
    "# Races table preliminary analysis\n",
    "Some columns have missing values in races table (**points**, **uci_points**, **climb_total**, **profile**, **average_temperature**, **cyclist_age**, **cycilist_team**).\n",
    "\n",
    "We can try to fix columns **points**, **cyclist_age** since there are few missing values.\n",
    "We can drop the column **average_temperature** since there are too many missing values.\n",
    "We can try to analyze the distribution of **climb_total** over **profile** to try to esitimate and fix the columns.\n",
    "We can assume that missing values if **cyclist_team** implies 'No team'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5d40c2-e081-4dd4-bca8-ff6a2e89fb73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:35.397218Z",
     "start_time": "2024-11-02T13:33:35.186403Z"
    }
   },
   "outputs": [],
   "source": [
    "races_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e4140-580f-4dcc-88ca-7ca9b510d081",
   "metadata": {},
   "source": [
    "There is some noise in the date we can remove the hours, minutes and seconds of start of the races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0debdf00-bd1b-4a05-9b09-928dab380d94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:37.580458Z",
     "start_time": "2024-11-02T13:33:36.858634Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's remove the noise from dates of races\n",
    "def reformat_dates(df):\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df[\"date\"].dt.date\n",
    "\n",
    "\n",
    "races_df[\"date\"] = reformat_dates(races_df)\n",
    "print(races_df[\"date\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f58a67",
   "metadata": {},
   "source": [
    "### Resolve NaN points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee41290-9cd0-4dc7-a05c-1f7f5d603d32",
   "metadata": {},
   "source": [
    "We can try to check if **points** column is mutually eclusive with **uci_points** column. They aren't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beafaef-3b6b-4086-8013-a103afbcbc83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:38.249113Z",
     "start_time": "2024-11-02T13:33:38.212110Z"
    }
   },
   "outputs": [],
   "source": [
    "# Points and UCI_Points are not mutually exclusive columns\n",
    "def are_xor_columns(df, col1, col2):\n",
    "    mask = df[col1].isna() ^ df[col2].isna()\n",
    "    res = df.loc[mask, \"cyclist\"].tolist()\n",
    "    return len(res) == len(df)\n",
    "\n",
    "\n",
    "print(are_xor_columns(races_df, \"points\", \"uci_points\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3367c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "races_df.loc[races_df.points.isna()]._url.unique()\n",
    "miss_points_dict = {\n",
    "    \"vuelta-a-espana/1994/stage-5\": 80,\n",
    "    \"tour-de-france/1986/stage-19\": 100,\n",
    "    \"tour-de-france/1988/prologue\": 100,\n",
    "    \"tour-de-france/2019/stage-19\": 100,\n",
    "}\n",
    "\n",
    "for k, v in miss_points_dict.items():\n",
    "    mask = races_df._url == k\n",
    "    races_df.loc[mask, \"points\"] = v\n",
    "\n",
    "# convert to int\n",
    "races_df[\"points\"] = races_df[\"points\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b5400",
   "metadata": {},
   "source": [
    "### Resolve is_tarmac (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112d72db-40b4-4ecc-b816-fed20b98594c",
   "metadata": {},
   "source": [
    "**is_cobbled** and **is_gravel** are all false (we can drop them).\n",
    "**is_tarmac** is sometimes true and false. It's ok but when it's false also the other columns are false (Noise!!). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc75499-bdce-44a7-b275-d68fd9626ff8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:39.589671Z",
     "start_time": "2024-11-02T13:33:39.554747Z"
    }
   },
   "outputs": [],
   "source": [
    "# Some columns are all False\n",
    "print(races_df.is_cobbled.unique())\n",
    "print(races_df.is_gravel.unique())\n",
    "print(races_df.is_tarmac.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8458e",
   "metadata": {},
   "source": [
    "### Resolve Deltas (DONE IN SCRAPING)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6147c62f",
   "metadata": {},
   "source": [
    "The delta column have some negative values and also the delta is 0 for many positions different from 0 (Very noisy column!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05846d42-025d-4929-b8b9-25b84ecba5c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:40.784816Z",
     "start_time": "2024-11-02T13:33:40.740481Z"
    }
   },
   "outputs": [],
   "source": [
    "# A column has inconsistent values. (Negative delta)\n",
    "print(sorted(races_df.delta.unique())[:100])\n",
    "# Replace negative values with nan\n",
    "races_df[\"delta\"] = races_df[\"delta\"].mask(races_df[\"delta\"] < 0, np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23329e53",
   "metadata": {},
   "source": [
    "### Resolve positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cfd45b",
   "metadata": {},
   "source": [
    "First position in a race is 0. We can check it looking on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b065369-d9f6-4d29-855e-9fd7e72f0d55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:42.261441Z",
     "start_time": "2024-11-02T13:33:42.228793Z"
    }
   },
   "outputs": [],
   "source": [
    "# First position in a race is 0. Delta is always 0 for position 0 and different from 0 for position. Checked using Internet\n",
    "delta0 = ((races_df[\"position\"] == 0) & (races_df[\"delta\"] == 0)).sum()\n",
    "delta1 = ((races_df[\"position\"] == 1) & (races_df[\"delta\"] == 0)).sum()\n",
    "print(delta0)\n",
    "print(delta1)\n",
    "\n",
    "# Much Much noise in delta column. All positions have some 0 deltas\n",
    "positions = sorted(races_df.loc[races_df[\"delta\"].isna(), \"position\"].unique())\n",
    "print(positions)\n",
    "positions = sorted(races_df.loc[races_df[\"delta\"] == 0, \"position\"].unique())\n",
    "print(positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e490bf20",
   "metadata": {},
   "source": [
    "Some positions are duplicate. since we cannot infer in any way which is the real one, remove both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ddf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "races_df.drop_duplicates(\n",
    "    subset=races_df.columns.difference([\"position\"]), keep=False, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abbd986",
   "metadata": {},
   "source": [
    "### Resolve profiles and total_climb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0019e0",
   "metadata": {},
   "source": [
    "Some races have missing profiles. Try to fix looking to the climb_total distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c761d15-bac3-4203-a865-ba4681e19613",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:43.782681Z",
     "start_time": "2024-11-02T13:33:43.751653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Profile is numerical (difficulty of trace) missing for some races\n",
    "print(races_df.profile.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3db09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(\n",
    "    data=races_df.groupby(\"_url\")[[\"profile\", \"climb_total\"]].first(),\n",
    "    x=\"profile\",\n",
    "    y=\"climb_total\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ca137",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for profile in [1, 2, 3, 4, 5]:\n",
    "    sns.histplot(\n",
    "        data=races_df.loc[races_df.profile == profile]\n",
    "        .groupby(\"_url\")[[\"profile\", \"climb_total\"]]\n",
    "        .first(),\n",
    "        x=\"climb_total\",\n",
    "        label=profile,\n",
    "    )\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8028cf",
   "metadata": {},
   "source": [
    "Look at correlation between the two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01a36b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_nan_df = races_df.loc[\n",
    "    (races_df.profile.notna() & races_df.climb_total.notna())\n",
    "]\n",
    "print(\"not nan urls\", not_nan_df._url.unique().shape)\n",
    "not_nan_df = not_nan_df.groupby(\"_url\")[\n",
    "    [\"_url\", \"profile\", \"climb_total\"]\n",
    "].first()\n",
    "print(races_df._url.unique().shape)\n",
    "print(not_nan_df._url.unique().shape)\n",
    "\n",
    "print(not_nan_df.profile.corr(not_nan_df.climb_total, method=\"spearman\"))\n",
    "print(not_nan_df.profile.corr(not_nan_df.climb_total, method=\"kendall\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720e3026",
   "metadata": {},
   "source": [
    "the two columns are correlated (spearman > 0.7, kendall > 0.55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a32980",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_races_df = races_df.groupby(\"_url\")[[\"profile\", \"climb_total\"]].first()\n",
    "print(\n",
    "    true_races_df.loc[\n",
    "        (true_races_df.profile.notna() & true_races_df.climb_total.isna())\n",
    "    ].shape\n",
    ")\n",
    "print(\n",
    "    true_races_df.loc[\n",
    "        (true_races_df.profile.isna() & true_races_df.climb_total.notna())\n",
    "    ].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a7c6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tried median imputation but it changes the distribution of the data\n",
    "# tried assigning value to class with closest median\n",
    "\"\"\"medians = []\n",
    "for i in range(1, 6):\n",
    "    mask = true_races_df.profile == i\n",
    "    medians.append(true_races_df.loc[mask, \"climb_total\"].median())\n",
    "\n",
    "mask = true_races_df.profile.isna() & true_races_df.climb_total.notna()\n",
    "temp_df = true_races_df.loc[mask]\n",
    "\n",
    "\n",
    "for i, row in temp_df.iterrows():\n",
    "    min_diff = float(\"inf\")\n",
    "    for j in range(5):\n",
    "        diff = abs(row[\"climb_total\"] - medians[j])\n",
    "        if diff < min_diff:\n",
    "            min_diff = diff\n",
    "            profile = j + 1\n",
    "\n",
    "    true_races_df.loc[i, \"profile\"] = profile\"\"\"\n",
    "\n",
    "fit_df = true_races_df.loc[\n",
    "    true_races_df.profile.notna() & true_races_df.climb_total.notna()\n",
    "]\n",
    "impute_df = true_races_df.loc[\n",
    "    true_races_df.profile.notna() & true_races_df.climb_total.isna()\n",
    "]\n",
    "\n",
    "imputer = imp.KNNImputer(n_neighbors=5)\n",
    "imputer.fit(fit_df)\n",
    "imputation = imputer.transform(impute_df)\n",
    "df_temp = pd.DataFrame(imputation, columns=[\"profile\", \"climb_total\"])\n",
    "df_temp.head()\n",
    "true_races_df.loc[\n",
    "    true_races_df.profile.notna() & true_races_df.climb_total.isna()\n",
    "] = df_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6acc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    true_races_df.loc[\n",
    "        (true_races_df.profile.notna() & true_races_df.climb_total.isna())\n",
    "    ].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e9daa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Profile is a categorical variable\n",
    "cat_imputer = imp.KNNImputer(n_neighbors=5)\n",
    "cat_imputer.fit(fit_df)\n",
    "impute_df = true_races_df.loc[\n",
    "    true_races_df.profile.isna() & true_races_df.climb_total.notna()\n",
    "]\n",
    "imputation = cat_imputer.transform(impute_df)\n",
    "imputation = np.round(imputation)\n",
    "df_temp = pd.DataFrame(imputation, columns=[\"profile\", \"climb_total\"])\n",
    "true_races_df.loc[\n",
    "    true_races_df.profile.isna() & true_races_df.climb_total.notna()\n",
    "] = df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007cdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    true_races_df.loc[\n",
    "        (true_races_df.profile.isna() & true_races_df.climb_total.notna())\n",
    "    ].shape\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31713dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how distribution changed\n",
    "plt.figure(figsize=(10, 12))\n",
    "plt.subplot(2, 1, 1)\n",
    "sns.boxplot(data=true_races_df, x=\"profile\", y=\"climb_total\")\n",
    "plt.subplot(2, 1, 2)\n",
    "for profile in [1, 2, 3, 4, 5]:\n",
    "    sns.histplot(\n",
    "        data=true_races_df.loc[true_races_df.profile == profile],\n",
    "        x=\"climb_total\",\n",
    "        label=profile,\n",
    "    )\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a1ae59",
   "metadata": {},
   "source": [
    "### Look startlist quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3cd36b-dd57-4f81-b386-3a04c48e93b0",
   "metadata": {},
   "source": [
    "The startlist are integers representing how strong the lineup is. They are sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ef62aa-3adf-4a63-a545-1aace14ddf7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:45.277914Z",
     "start_time": "2024-11-02T13:33:45.242664Z"
    }
   },
   "outputs": [],
   "source": [
    "races_df.startlist_quality.unique()[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bed373-3b05-4b4a-b9a6-fb4465d75f84",
   "metadata": {},
   "source": [
    "Now we can build the joined table and analyze the data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23814a28-0375-49b9-9a6a-a31c1336fa56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:46.896079Z",
     "start_time": "2024-11-02T13:33:46.873586Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataUnderstander:\n",
    "    def __init__(self, races_csv, cyclist_csv, merged_csv=None):\n",
    "        # So keep ref to all datasets and make one joined\n",
    "        self.races_df = races_csv\n",
    "        self.cyclist_df = cyclist_csv\n",
    "\n",
    "        if merged_csv == None:\n",
    "            self.df = pd.merge(\n",
    "                self.races_df,\n",
    "                self.cyclist_df,\n",
    "                left_on=\"cyclist\",\n",
    "                right_on=\"_url\",\n",
    "                how=\"inner\",\n",
    "            )\n",
    "        else:\n",
    "            self.df = pd.read_csv(merged_csv, parse_dates=[\"date\"])\n",
    "\n",
    "        # Delete useless columns for the joined table\n",
    "        self.delete_column(\"_url_y\")\n",
    "        self.delete_column(\"name_y\")\n",
    "\n",
    "        # Delete columns that are all false\n",
    "        self.delete_column(\"is_cobbled\")\n",
    "        self.delete_column(\"is_gravel\")\n",
    "\n",
    "        # Rename to understand better\n",
    "        self.df.rename(\n",
    "            columns={\n",
    "                \"name_x\": \"location\",\n",
    "                \"_url_x\": \"url\",\n",
    "            },\n",
    "            inplace=True,\n",
    "        )\n",
    "\n",
    "        # dm.export_csv() # Save the merged version of dataset\n",
    "\n",
    "    def delete_column(self, col):\n",
    "        self.df.drop(columns=[col], inplace=True)\n",
    "\n",
    "    def fill_nan(self, column, value):\n",
    "        self.df[column].fillna(value, inplace=True)\n",
    "\n",
    "    def export_csv(self):\n",
    "        return self.df.to_csv(\"./dataset/dataset.csv\")\n",
    "\n",
    "    def correlate(self):\n",
    "        correlations = {\n",
    "            correlation_type: self.df.corr(\n",
    "                numeric_only=True, method=correlation_type\n",
    "            )\n",
    "            for correlation_type in (\"kendall\", \"pearson\", \"spearman\")\n",
    "        }\n",
    "\n",
    "        for i, k in enumerate(correlations.keys()):\n",
    "            correlations[k].loc[:, \"correlation_type\"] = k\n",
    "\n",
    "        # Loop through each correlation type and create separate plots\n",
    "        for corr_type, corr_matrix in correlations.items():\n",
    "            corr_matrix = corr_matrix.drop(columns=[\"correlation_type\"])\n",
    "\n",
    "            # Create a new figure for each correlation type\n",
    "            plt.figure(figsize=(12, 12))\n",
    "\n",
    "            # Plot the heatmap\n",
    "            sns.heatmap(\n",
    "                corr_matrix,\n",
    "                annot=True,\n",
    "                cmap=\"coolwarm\",\n",
    "                fmt=\".2f\",\n",
    "                linewidths=0.5,\n",
    "            )\n",
    "\n",
    "            # Set the title for the plot\n",
    "            plt.title(f\"{corr_type.capitalize()} Correlation\")\n",
    "\n",
    "            # Display the plot\n",
    "            plt.show()\n",
    "\n",
    "    def normalize(self):\n",
    "        numeric_columns = [\n",
    "            \"points\",\n",
    "            \"length\",\n",
    "            \"climb_total\",\n",
    "            \"profile\",\n",
    "            \"startlist_quality\",\n",
    "            \"cyclist_age\",\n",
    "            \"delta\",\n",
    "            \"birth_year\",\n",
    "            \"weight\",\n",
    "            \"height\",\n",
    "        ]\n",
    "        scaler = StandardScaler()  # Si prova con la zscore ora eh\n",
    "        self.df[numeric_columns] = scaler.fit_transform(\n",
    "            self.df[numeric_columns]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70905c13-5887-48df-9909-6a19115aed7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:33:49.835366Z",
     "start_time": "2024-11-02T13:33:48.788833Z"
    }
   },
   "outputs": [],
   "source": [
    "dm = DataUnderstander(races_df, cyclist_df)  # Initialize the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91071437-cbcc-4979-a8f6-b25649c668ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:34:41.562287Z",
     "start_time": "2024-11-02T13:34:41.352361Z"
    }
   },
   "outputs": [],
   "source": [
    "dm.df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0779ac-fd41-45be-9277-c8217fc72cc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:34:42.986488Z",
     "start_time": "2024-11-02T13:34:42.040982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Try to fix missing birth_year or age using the other column. Nothing they always nan at the same time\n",
    "\n",
    "ages = dm.df.groupby(\"cyclist\")[\"cyclist_age\"].unique()\n",
    "\n",
    "ages = ages.apply(lambda x: np.isnan(x).all())\n",
    "# names for which is true\n",
    "ages = ages[ages == True].index\n",
    "\n",
    "for idx, name in enumerate(ages):\n",
    "    print(idx, name)\n",
    "    print(cyclist_df.loc[cyclist_df._url == name].birth_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527a2a2",
   "metadata": {},
   "source": [
    "# Distribution analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79634983bebe6b6e",
   "metadata": {},
   "source": [
    "To analyze the distribution the distribution we need to work on merged table but also on individual tables.\n",
    "- **Cyclists table** is already ready.\n",
    "- **Races table** is not ready. we need to group by race to analyze the attribute relative to a race.\n",
    "- **Merged table** is not ready. we need to throw away columns relative to a single race (static attributes)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64ab8dff5a1246",
   "metadata": {},
   "source": [
    "First of all let's retrieve the race table without duplicates of static attributes and plot the distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a98e1e79f6b74b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:50:19.471219Z",
     "start_time": "2024-11-02T13:50:18.180049Z"
    }
   },
   "outputs": [],
   "source": [
    "grouped_races_df = races_df.groupby(\n",
    "    [\n",
    "        \"_url\",\n",
    "    ]\n",
    ")[\n",
    "    [\n",
    "        \"_url\",\n",
    "        \"name\",\n",
    "        \"points\",\n",
    "        \"uci_points\",\n",
    "        \"length\",\n",
    "        \"climb_total\",\n",
    "        \"profile\",\n",
    "        \"startlist_quality\",\n",
    "        \"date\",\n",
    "    ]\n",
    "].apply(lambda x: x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb4b232",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T13:59:36.524504Z",
     "start_time": "2024-11-02T13:59:32.005732Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot(df, col, *args, **kwargs):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.subplot(2, 2, 1)  # (rows, columns, index)\n",
    "    sns.histplot(x=col, data=df, *args, **kwargs)\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.subplot(2, 2, 2)  # (rows, columns, index)\n",
    "    sns.boxplot(x=col, data=df)\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot(grouped_races_df, \"points\")\n",
    "plot(grouped_races_df, \"uci_points\")\n",
    "plot(grouped_races_df, \"length\")\n",
    "plot(grouped_races_df, \"climb_total\")\n",
    "plot(grouped_races_df, \"startlist_quality\")\n",
    "plot(grouped_races_df, \"date\")\n",
    "plot(grouped_races_df, \"profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4ef29",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(dm.df[\"position\"], dm.df[\"delta\"])\n",
    "plt.xlabel(\"Position\")\n",
    "plt.ylabel(\"Delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8477883d",
   "metadata": {},
   "source": [
    "Now we can do the same for the cyclist table. In his case we have also some categorical attributes to plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f7aeac00d42c1c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-02T14:22:55.811412Z",
     "start_time": "2024-11-02T14:22:52.755186Z"
    }
   },
   "outputs": [],
   "source": [
    "plot(cyclist_df, \"birth_year\")\n",
    "plot(cyclist_df, \"weight\")\n",
    "plot(cyclist_df, \"height\")\n",
    "\n",
    "\n",
    "# Nationalities plot is very big. Let's split in two figures\n",
    "def plot(df, col):\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.histplot(x=col, data=df)\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.show()\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.boxplot(x=col, data=df)\n",
    "    plt.xticks(rotation=90, ha=\"right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot(cyclist_df, \"nationality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c5ab2c0133f24f",
   "metadata": {},
   "source": [
    "Now we can analyze the attributes that are related both on race and on cyclist (Ex position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f7cc4-939e-4916-96e4-3bb0a253b1ce",
   "metadata": {},
   "source": [
    "Now we can build the build the correlation matrix to try to understand which columns are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516005c4-78d6-4045-b520-bc7a51a53bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm.normalize()\n",
    "dm.correlate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da26b781-f62a-4f27-8ec4-8cb6e35825d7",
   "metadata": {},
   "source": [
    "In all the 3 plots we can observe:\n",
    "- We can observe that the columns **climb_total** and **profile** are highly correlated, so we can use them to fix the missing values.\n",
    "- We can observe that weight and height are highly correlated but we can expect this because it's naturally true.\n",
    "- We can observe that **points** and **uci_points** are highly correlated.\n",
    "- We can observe that **profile** and **delta** are correlated.\n",
    "- We can observe that **climb_total** and **delta** are correlated.\n",
    "- We can observe that **startlist_quality** and **points** are correlated.\n",
    "- We can observe that **length** and **points** are correlated.\n",
    "- We can observe that **birth_year** and **uci_points** are correlated.\n",
    "Note: Normalizing the data does not affect the correlation matrix."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
